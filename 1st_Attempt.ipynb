{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import time \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the corpus excel file that contains the detail of all text files\n",
    "final_list=pd.read_excel(\"corpus-final09.xls\",sheet_name=\"File list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Input:\n",
    "    def __init__(self,directory=os.getcwd()):\n",
    "        self.directory=directory\n",
    "        self.all_words=[]\n",
    "        self.inverted_index={}\n",
    "        self.doc_sim_score={}\n",
    "        self.all_files={}\n",
    "        self.dict_list={}\n",
    "        self.dict_lemm_or_stem={}\n",
    "        self.ultimate_sim=[]\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    # Function to retrieve all the documents from the directory that is given as a input when a class Input object is initialized\n",
    "    def retrieve_file(self,file_name=None,encoding=\"utf-8\"):\n",
    "        \n",
    "        for _,filename in enumerate(os.listdir(self.directory)):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                f = io.open(filename, mode=\"r\", encoding=encoding)\n",
    "                lines = f.read()\n",
    "                self.all_files[filename.replace('.txt','')]=re.sub('[^a-zA-Z0-9]', ' ', lines.rstrip())\n",
    "                #self.all_files is a dict that contains all the text present in a word doc in the format{\"FILE_NAME\":\"TEXT IN THE FILE\"}\n",
    "            \n",
    "    \n",
    "    def tok_lem_stem(self,type_op=None):\n",
    "        #type_op can be a list also in the case when both lemmatization and stemming is to be applied \n",
    "        # Preprocessing the document and applying Case Folding, rudimernary normalization and spliting string to words and performing lemmitization of stemming based on user input\n",
    "        self.operation=type_op\n",
    "        for key in self.all_files.keys():\n",
    "            lemmatizer=WordNetLemmatizer()\n",
    "            stemmer=PorterStemmer()\n",
    "            self.dict_list[key]=[]\n",
    "            [self.dict_list[key].append(x) for x in self.all_files[key].lower().split(\" \") if x not in self.stop_words if x is not '']\n",
    "            # self.dict_list is a processed version of self.all_files with case folding done and stop words removed\n",
    "            \n",
    "            #To perform lemmatization of stemming based on user input\n",
    "            for key in self.dict_list.keys():\n",
    "                self.dict_lemm_or_stem[key]=[]\n",
    "                if 'lemmatize' in type_op:\n",
    "                    [self.dict_lemm_or_stem[key].append(lemmatizer.lemmatize(x)) for x in self.dict_list[key]]\n",
    "                elif 'stemming' in type_op:\n",
    "                    [self.dict_lemm_or_stem[key].append(stemmer.stem(x)) for x in self.dict_list[key]]\n",
    "\n",
    "                self.dict_lemm_or_stem[key]=[x for x in self.dict_lemm_or_stem[key] if x is not '']\n",
    "           \n",
    "    \n",
    "    def preprocess_query_doc(self,filename,encoding=\"utf-8\"):\n",
    "        \n",
    "        #Preprocess the query document in the same order as the corpus was preprocessed\n",
    "        \n",
    "        f = io.open(filename, mode=\"r\", encoding=encoding)\n",
    "        lines = f.read()\n",
    "        process_string=re.sub('[^a-zA-Z0-9]', ' ', lines.rstrip())\n",
    "        string_updated=[x for x in process_string.lower().split(\" \") if x not in self.stop_words if x is not '']\n",
    "        lemmatizer=WordNetLemmatizer()\n",
    "        stemmer=PorterStemmer()\n",
    "        if 'lemmatize' in self.operation:\n",
    "            result=[lemmatizer.lemmatize(x) for x in string_updated]\n",
    "        elif 'stemming' in self.operation:\n",
    "            result=[stemmer.stem(x) for x in string_updated]\n",
    "\n",
    "        result=[x for x in result if x is not '']\n",
    "        return result\n",
    "    \n",
    "    def inverted_index_constr(self):\n",
    "        # Constructing Inverted Index of the format {\"WORD1\":[DOC1,DOC3,...],\"WORD2\":[...]}\n",
    "        if self.operation:\n",
    "            text_dict=self.dict_lemm_or_stem\n",
    "        else:\n",
    "            text_dict=self.dict_list\n",
    "            \n",
    "        for key in text_dict.keys():\n",
    "            [self.all_words.append(x) for x in text_dict[key] if x not in self.all_words]\n",
    "            \n",
    "        for word in self.all_words:\n",
    "            self.inverted_index[word]=[]\n",
    "            [self.inverted_index[word].append(key) for key in self.all_files.keys() if word in self.dict_lemm_or_stem[key]]\n",
    "\n",
    "    \n",
    "    def calculate_tf_idf(self,test_file=None,encoding_test=\"utf-8\"):\n",
    "        \n",
    "        #Calculate tf-idf score\n",
    "        test_string=self.preprocess_query_doc(filename=test_file,encoding=encoding_test)\n",
    "        for word in test_string:\n",
    "            self.doc_sim_score[word]=[]\n",
    "            try:\n",
    "                doc_having_word=len(self.inverted_index[word])\n",
    "            except:\n",
    "                doc_having_word=0\n",
    "            [self.doc_sim_score[word].append([doc,self.dict_lemm_or_stem[doc].count(word)/len(self.dict_lemm_or_stem[doc])*np.log(len(self.all_files)/(doc_having_word+1))]) for doc in self.all_files.keys()]\n",
    "            \n",
    "            \n",
    "    def ultimate_sim_score(self):\n",
    "        #Compute the final similarity score by adding the similarity score of each word over all the documents and ranking them according to highest similarity\n",
    "        for key in self.all_files.keys():\n",
    "            self.ultimate_sim.append([key,0])\n",
    "        for word in self.doc_sim_score.keys():\n",
    "            for i,key in enumerate(self.all_files.keys()):\n",
    "                self.ultimate_sim[i][1]=self.ultimate_sim[i][1]+self.doc_sim_score[word][i][1]\n",
    "        def takeSecond(elem):\n",
    "            return elem[1]\n",
    "        \n",
    "        return sorted(self.ultimate_sim,key=lambda x:x[1],reverse=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.148210048675537\n",
      "0.13292264938354492\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "inn=Input()\n",
    "inn.retrieve_file()\n",
    "inn.tok_lem_stem(type_op='lemmatize')\n",
    "inn.inverted_index_constr()\n",
    "endtime=time.time()\n",
    "print(endtime-start)\n",
    "start=time.time()\n",
    "inn.calculate_tf_idf(test_file='orig_taskb.txt')\n",
    "endtime=time.time()\n",
    "print(endtime-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['orig_taskb', 2.287509701630022],\n",
       " ['g4pE_taskb', 2.228414320071634],\n",
       " ['g0pA_taskb', 2.0886298705404016],\n",
       " ['g2pE_taskb', 2.019319553291541],\n",
       " ['g0pD_taskb', 1.8394836786555568],\n",
       " ['g3pA_taskb', 1.580601136063834],\n",
       " ['g0pE_taskb', 1.393950632627336],\n",
       " ['g2pA_taskb', 1.1191342128403126],\n",
       " ['g1pD_taskb', 1.0565456965777569],\n",
       " ['g0pB_taskb', 1.0511404520359449],\n",
       " ['g4pC_taskb', 0.9858954088942519],\n",
       " ['g2pC_taskb', 0.9497566654959324],\n",
       " ['g1pA_taskb', 0.8975073624639582],\n",
       " ['g2pB_taskb', 0.8351710258380014],\n",
       " ['g3pC_taskb', 0.7722905086924529],\n",
       " ['g1pB_taskb', 0.7421376366628747],\n",
       " ['g4pB_taskb', 0.7158653573579165],\n",
       " ['g3pB_taskb', 0.6846952421334103],\n",
       " ['g0pC_taskb', 0.6719593318492858],\n",
       " ['g4pD_taskb', 0.6666606451398052],\n",
       " ['g0pE_taskd', 0.5397109152356317],\n",
       " ['g0pB_taskd', 0.5066445124482073],\n",
       " ['g4pB_taskd', 0.4617667603966141],\n",
       " ['g3pC_taskd', 0.42556496410554606],\n",
       " ['g3pA_taskd', 0.4116947955517746],\n",
       " ['g4pC_taskd', 0.41099818414656003],\n",
       " ['g2pC_taskd', 0.4095015026592429],\n",
       " ['orig_taskd', 0.40463717048517267],\n",
       " ['g2pB_taskd', 0.40452473775992503],\n",
       " ['g1pB_taskd', 0.4002488183083331],\n",
       " ['g1pA_taskc', 0.39472513323877484],\n",
       " ['g3pC_taskc', 0.39440635230023385],\n",
       " ['g1pD_taskd', 0.38965690674638015],\n",
       " ['g0pD_taskd', 0.37646723565923235],\n",
       " ['g2pE_taskc', 0.34328743187104793],\n",
       " ['g4pC_taskc', 0.32157167133332815],\n",
       " ['g3pB_taskc', 0.32023333585242403],\n",
       " ['g2pE_taskd', 0.3197001342031839],\n",
       " ['g4pE_taskd', 0.3172638046035991],\n",
       " ['g1pD_taskc', 0.3143887148911573],\n",
       " ['g1pB_taskc', 0.30830412959671105],\n",
       " ['g4pD_taskc', 0.30312082549076996],\n",
       " ['g0pA_taskc', 0.28804765481140754],\n",
       " ['g4pD_taska', 0.2847463991261601],\n",
       " ['g2pA_taskd', 0.2826511459725887],\n",
       " ['g0pB_taskc', 0.2819270464868598],\n",
       " ['g4pE_taskc', 0.2817442650809746],\n",
       " ['g2pE_taska', 0.28137572762245844],\n",
       " ['g4pB_taska', 0.27748667826568535],\n",
       " ['g0pE_taska', 0.2766109075195848],\n",
       " ['orig_taska', 0.27438282112479195],\n",
       " ['g3pA_taskc', 0.2722162056383793],\n",
       " ['g2pB_taskc', 0.26743545767620625],\n",
       " ['g2pA_taskc', 0.26315081811858937],\n",
       " ['orig_taskc', 0.24837311624414715],\n",
       " ['g0pC_taskd', 0.24746137719124203],\n",
       " ['g0pE_taskc', 0.24548404615283137],\n",
       " ['g0pD_taskc', 0.2380524649648857],\n",
       " ['g4pC_taska', 0.23741181648698154],\n",
       " ['g2pB_taska', 0.23725224915626691],\n",
       " ['g1pA_taskd', 0.23595157715077084],\n",
       " ['g2pC_taskc', 0.21739019113242353],\n",
       " ['g0pD_taske', 0.21131492320264966],\n",
       " ['g3pB_taskd', 0.20732463072605578],\n",
       " ['g1pA_taske', 0.20399570214497198],\n",
       " ['g0pA_taske', 0.19401965064018722],\n",
       " ['g4pB_taskc', 0.19058853680808294],\n",
       " ['g4pE_taske', 0.18731433082206683],\n",
       " ['g1pB_taska', 0.1861615709154941],\n",
       " ['g2pA_taske', 0.18574646954512444],\n",
       " ['g0pD_taska', 0.17985456508937644],\n",
       " ['g0pC_taskc', 0.17856844999159033],\n",
       " ['g0pA_taskd', 0.17094223634531774],\n",
       " ['g3pA_taske', 0.1657754527930836],\n",
       " ['g3pC_taska', 0.16382532895049023],\n",
       " ['g2pC_taska', 0.16028027511164317],\n",
       " ['g1pB_taske', 0.15871063584036982],\n",
       " ['g4pE_taska', 0.15633766149896247],\n",
       " ['g2pB_taske', 0.14762065119107584],\n",
       " ['g1pD_taska', 0.1414395261178648],\n",
       " ['g4pB_taske', 0.14119792789045074],\n",
       " ['g2pE_taske', 0.1392682584535964],\n",
       " ['g4pD_taskd', 0.13787176781302113],\n",
       " ['g4pD_taske', 0.13507347575382667],\n",
       " ['g0pB_taske', 0.13240814410650245],\n",
       " ['g1pD_taske', 0.12858089352898244],\n",
       " ['orig_taske', 0.12763201722264647],\n",
       " ['g0pC_taska', 0.1261519478822604],\n",
       " ['g4pC_taske', 0.1193820721464182],\n",
       " ['g2pA_taska', 0.11643428984907947],\n",
       " ['g0pB_taska', 0.10937462723342617],\n",
       " ['g0pA_taska', 0.10876859094865113],\n",
       " ['g0pE_taske', 0.0888605426797001],\n",
       " ['g1pA_taska', 0.08716349122806398],\n",
       " ['g3pA_taska', 0.08480169017170852],\n",
       " ['g2pC_taske', 0.08434437231792322],\n",
       " ['g3pB_taska', 0.08430358776471517],\n",
       " ['g0pC_taske', 0.08059296334296727],\n",
       " ['g3pB_taske', 0.07872161817846894],\n",
       " ['g3pC_taske', 0.05401877903819868]]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inn.ultimate_sim_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
